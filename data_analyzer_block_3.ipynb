{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAMYN9U//wr4zl1JAup3VO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pendekantejeevan760-cpu/data-analyzer-module/blob/main/data_analyzer_block_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHQgAAMbBkWN",
        "outputId": "a0df80d4-5112-4a49-e3f7-bf85e35755c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies and imports\n",
        "!pip install pillow scikit-learn tensorflow pandas numpy\n",
        "\n",
        "import json\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from enum import Enum\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "from PIL.ExifTags import TAGS, GPSTAGS\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "import os\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 FIXED: Enums and Data Classes with proper JSON serialization\n",
        "\n",
        "class DisasterType(Enum):\n",
        "    TSUNAMI = \"tsunami\"\n",
        "    HURRICANE = \"hurricane\"\n",
        "    STORM_SURGE = \"storm_surge\"\n",
        "    COASTAL_FLOODING = \"coastal_flooding\"\n",
        "    RIP_CURRENT = \"rip_current\"\n",
        "    HIGH_WAVES = \"high_waves\"\n",
        "    ALGAE_BLOOM = \"algae_bloom\"\n",
        "    OIL_SPILL = \"oil_spill\"\n",
        "    EROSION = \"coastal_erosion\"\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "\n",
        "class AlertLevel(Enum):\n",
        "    WARNING = 1\n",
        "    EMERGENCY = 2\n",
        "    EVACUATION = 3\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DisasterReport:\n",
        "    disaster_type: DisasterType\n",
        "    confidence_score: float\n",
        "    reliability_score: float\n",
        "    alert_level: AlertLevel\n",
        "    location: Dict[str, float]\n",
        "    timestamp: datetime\n",
        "    description: str\n",
        "    image_analysis: Dict\n",
        "    schema: Dict\n",
        "    report_id: Optional[str] = None\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary with proper serialization of enums and datetime\"\"\"\n",
        "        return {\n",
        "            \"disaster_type\": self.disaster_type.value if self.disaster_type else \"unknown\",\n",
        "            \"confidence_score\": self.confidence_score,\n",
        "            \"reliability_score\": self.reliability_score,\n",
        "            \"alert_level\": self.alert_level.value if self.alert_level else 1,\n",
        "            \"alert_level_name\": self.alert_level.name if self.alert_level else \"WARNING\",\n",
        "            \"location\": self.location or {\"lat\": 0.0, \"lon\": 0.0},\n",
        "            \"timestamp\": self.timestamp.isoformat() if self.timestamp else datetime.now().isoformat(),\n",
        "            \"description\": self.description or \"\",\n",
        "            \"image_analysis\": self.image_analysis or {},\n",
        "            \"schema\": self.schema or {},\n",
        "            \"report_id\": self.report_id\n",
        "        }\n",
        "\n",
        "# Helper function to prevent NoneType errors\n",
        "def safe_len(obj):\n",
        "    \"\"\"Safe length function that handles None values\"\"\"\n",
        "    return len(obj) if obj is not None else 0\n",
        "\n",
        "# Custom JSON encoder for handling remaining enum serialization issues\n",
        "class DisasterJSONEncoder(json.JSONEncoder):\n",
        "    \"\"\"Custom JSON encoder that can handle DisasterType and AlertLevel enums\"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, DisasterType):\n",
        "            return obj.value\n",
        "        elif isinstance(obj, AlertLevel):\n",
        "            return obj.value\n",
        "        elif isinstance(obj, datetime):\n",
        "            return obj.isoformat()\n",
        "        return super().default(obj)\n",
        "\n",
        "print(\"Fixed enums and data classes with proper JSON serialization defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaNpaycMBnUt",
        "outputId": "aa984144-d2b7-4a95-fc5e-2ac0ae24128f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed enums and data classes with proper JSON serialization defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: OceanDisasterDetector Class - Initialization and Setup Methods\n",
        "\n",
        "class OceanDisasterDetector:\n",
        "    def __init__(self, database_path: str, output_path: str = \"disaster_reports.json\"):\n",
        "        \"\"\"\n",
        "        Initialize the detector with user-provided database.\n",
        "        \"\"\"\n",
        "        self.database_path = database_path\n",
        "        self.output_path = output_path\n",
        "        self.disaster_keywords = self._initialize_disaster_keywords()\n",
        "        self.vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "\n",
        "        try:\n",
        "            self.db_connection = sqlite3.connect(database_path)\n",
        "            self._validate_database_schema()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during analysis: {e}\")\n",
        "            raise\n",
        "\n",
        "    def close_connection(self):\n",
        "        if hasattr(self, \"db_connection\"):\n",
        "            self.db_connection.close()\n",
        "\n",
        "    def _initialize_disaster_keywords(self) -> Dict[DisasterType, List[str]]:\n",
        "        return {\n",
        "            DisasterType.TSUNAMI: [\n",
        "                \"tsunami\", \"tidal wave\", \"seismic sea wave\", \"massive wave\",\n",
        "                \"water rushing inland\", \"sea level rise\", \"earthquake wave\",\n",
        "            ],\n",
        "            DisasterType.HURRICANE: [\n",
        "                \"hurricane\", \"cyclone\", \"typhoon\", \"storm\", \"wind damage\",\n",
        "                \"rotating storm\", \"eye wall\", \"storm system\",\n",
        "            ],\n",
        "            DisasterType.STORM_SURGE: [\n",
        "                \"storm surge\", \"surge flooding\", \"sea level surge\",\n",
        "                \"coastal inundation\", \"storm tide\", \"water surge\",\n",
        "            ],\n",
        "            DisasterType.COASTAL_FLOODING: [\n",
        "                \"coastal flooding\", \"sea flooding\", \"tidal flooding\",\n",
        "                \"water intrusion\", \"flood water\", \"inundation\",\n",
        "            ],\n",
        "            DisasterType.RIP_CURRENT: [\n",
        "                \"rip current\", \"dangerous current\", \"strong undertow\",\n",
        "                \"water pulling\", \"current danger\", \"swimming hazard\",\n",
        "            ],\n",
        "            DisasterType.HIGH_WAVES: [\n",
        "                \"high waves\", \"large waves\", \"dangerous waves\",\n",
        "                \"wave height\", \"surf danger\", \"wave action\",\n",
        "            ],\n",
        "            DisasterType.ALGAE_BLOOM: [\n",
        "                \"algae bloom\", \"red tide\", \"harmful algae\",\n",
        "                \"water discoloration\", \"toxic algae\", \"bloom event\",\n",
        "            ],\n",
        "            DisasterType.OIL_SPILL: [\n",
        "                \"oil spill\", \"petroleum leak\", \"oil contamination\",\n",
        "                \"oil slick\", \"marine pollution\", \"oil discharge\",\n",
        "            ],\n",
        "            DisasterType.EROSION: [\n",
        "                \"coastal erosion\", \"beach erosion\", \"shoreline retreat\",\n",
        "                \"cliff collapse\", \"sand loss\", \"erosion damage\",\n",
        "            ],\n",
        "        }\n",
        "\n",
        "    def _validate_database_schema(self):\n",
        "        cursor = self.db_connection.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = [row[0] for row in cursor.fetchall()]\n",
        "        required_tables = [\"reports\", \"images\", \"locations\"]\n",
        "        if not all(table in tables for table in required_tables):\n",
        "            print(\"Creating expected database schema...\")\n",
        "            self._create_expected_schema()\n",
        "        else:\n",
        "            print(\"Database schema validated successfully\")\n",
        "\n",
        "    def _create_expected_schema(self):\n",
        "        cursor = self.db_connection.cursor()\n",
        "\n",
        "        # Create all required tables\n",
        "        schema_queries = [\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS reports (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                report_id TEXT UNIQUE,\n",
        "                description TEXT,\n",
        "                disaster_type TEXT,\n",
        "                timestamp TEXT,\n",
        "                status TEXT DEFAULT 'pending',\n",
        "                user_id TEXT,\n",
        "                created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
        "                error_message TEXT\n",
        "            )\"\"\",\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                report_id TEXT,\n",
        "                image_data TEXT,\n",
        "                image_path TEXT,\n",
        "                filename TEXT,\n",
        "                has_gps BOOLEAN DEFAULT 0,\n",
        "                gps_lat REAL,\n",
        "                gps_lon REAL,\n",
        "                created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
        "                FOREIGN KEY (report_id) REFERENCES reports (report_id)\n",
        "            )\"\"\",\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS locations (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                report_id TEXT,\n",
        "                latitude REAL,\n",
        "                longitude REAL,\n",
        "                location_source TEXT,\n",
        "                address TEXT,\n",
        "                is_coastal BOOLEAN,\n",
        "                accuracy REAL,\n",
        "                created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
        "                FOREIGN KEY (report_id) REFERENCES reports (report_id)\n",
        "            )\"\"\",\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS historical_disasters (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                disaster_type TEXT,\n",
        "                latitude REAL,\n",
        "                longitude REAL,\n",
        "                description TEXT,\n",
        "                severity INTEGER,\n",
        "                timestamp TEXT,\n",
        "                source TEXT,\n",
        "                created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
        "            )\"\"\",\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS users (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                user_id TEXT UNIQUE,\n",
        "                username TEXT,\n",
        "                email TEXT,\n",
        "                reliability_score REAL DEFAULT 0.5,\n",
        "                total_reports INTEGER DEFAULT 0,\n",
        "                verified_reports INTEGER DEFAULT 0,\n",
        "                created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
        "            )\"\"\",\n",
        "            \"\"\"CREATE TABLE IF NOT EXISTS analysis_results (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                report_id TEXT UNIQUE,\n",
        "                disaster_type TEXT,\n",
        "                confidence_score REAL,\n",
        "                reliability_score REAL,\n",
        "                alert_level INTEGER,\n",
        "                location_lat REAL,\n",
        "                location_lon REAL,\n",
        "                analysis_data TEXT,\n",
        "                processed_at TEXT DEFAULT CURRENT_TIMESTAMP,\n",
        "                FOREIGN KEY (report_id) REFERENCES reports (report_id)\n",
        "            )\"\"\"\n",
        "        ]\n",
        "\n",
        "        for query in schema_queries:\n",
        "            cursor.execute(query)\n",
        "\n",
        "        self.db_connection.commit()\n",
        "        print(\"Database schema created successfully\")\n",
        "\n",
        "print(\"OceanDisasterDetector initialization methods defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcU1tLxwBtpN",
        "outputId": "79a9d696-2432-4d7a-fbbd-a936a0b5708d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OceanDisasterDetector initialization methods defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 FIXED: Data Loading and Database Methods with proper JSON serialization\n",
        "\n",
        "# Add these methods to OceanDisasterDetector class\n",
        "def load_pending_reports(self) -> List[Dict[str, Any]]:\n",
        "    cursor = self.db_connection.cursor()\n",
        "    query = \"\"\"\n",
        "        SELECT\n",
        "            r.report_id,\n",
        "            r.description,\n",
        "            r.disaster_type,\n",
        "            r.timestamp,\n",
        "            r.user_id,\n",
        "            l.latitude,\n",
        "            l.longitude,\n",
        "            l.location_source,\n",
        "            l.address,\n",
        "            l.is_coastal\n",
        "        FROM reports r\n",
        "        LEFT JOIN locations l ON r.report_id = l.report_id\n",
        "        WHERE r.status = 'pending'\n",
        "        ORDER BY r.created_at\n",
        "    \"\"\"\n",
        "    cursor.execute(query)\n",
        "    reports = cursor.fetchall()\n",
        "\n",
        "    report_data: List[Dict[str, Any]] = []\n",
        "    for report in reports:\n",
        "        report_dict = {\n",
        "            \"report_id\": report[0],\n",
        "            \"description\": report[1] or \"\",\n",
        "            \"selected_disaster\": report[2],\n",
        "            \"timestamp\": report[3],\n",
        "            \"user_id\": report[4],\n",
        "            \"location\": {\n",
        "                \"lat\": report[5],\n",
        "                \"lon\": report[6],\n",
        "                \"source\": report[7],\n",
        "                \"address\": report[8],\n",
        "                \"is_coastal\": bool(report[9]) if report[9] is not None else None,\n",
        "            }\n",
        "            if report[5] is not None and report[6] is not None\n",
        "            else None,\n",
        "            \"images\": self._load_images_for_report(report[0]),\n",
        "        }\n",
        "        report_data.append(report_dict)\n",
        "\n",
        "    return report_data\n",
        "\n",
        "def _load_images_for_report(self, report_id: str) -> List[str]:\n",
        "    if not report_id:\n",
        "        return []\n",
        "\n",
        "    cursor = self.db_connection.cursor()\n",
        "    cursor.execute(\n",
        "        \"\"\"\n",
        "        SELECT image_data, image_path, has_gps, gps_lat, gps_lon\n",
        "        FROM images\n",
        "        WHERE report_id = ?\n",
        "        \"\"\",\n",
        "        (report_id,),\n",
        "    )\n",
        "    images_data = cursor.fetchall()\n",
        "    images: List[str] = []\n",
        "\n",
        "    for img_data in images_data:\n",
        "        image_b64, image_path, has_gps, gps_lat, gps_lon = img_data\n",
        "        if image_b64:\n",
        "            images.append(image_b64)\n",
        "        elif image_path and os.path.exists(image_path):\n",
        "            try:\n",
        "                with open(image_path, \"rb\") as f:\n",
        "                    img_bytes = f.read()\n",
        "                    img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
        "                    images.append(img_b64)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image from path {image_path}: {e}\")\n",
        "\n",
        "    return images\n",
        "\n",
        "def _update_report_status(self, report_id: Optional[str], status: str, error_msg: Optional[str] = None):\n",
        "    if not report_id:\n",
        "        return\n",
        "    cursor = self.db_connection.cursor()\n",
        "    if error_msg:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            UPDATE reports\n",
        "            SET status = ?, error_message = ?\n",
        "            WHERE report_id = ?\n",
        "            \"\"\",\n",
        "            (status, error_msg, report_id),\n",
        "        )\n",
        "    else:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            UPDATE reports\n",
        "            SET status = ?\n",
        "            WHERE report_id = ?\n",
        "            \"\"\",\n",
        "            (status, report_id),\n",
        "        )\n",
        "    self.db_connection.commit()\n",
        "\n",
        "def _save_analysis_result(self, report: DisasterReport):\n",
        "    \"\"\"FIXED: Save analysis result with proper enum serialization\"\"\"\n",
        "    if not report:\n",
        "        return\n",
        "\n",
        "    cursor = self.db_connection.cursor()\n",
        "\n",
        "    # Use custom JSON encoder to handle enums properly\n",
        "    analysis_data = json.dumps({\n",
        "        \"image_analysis\": report.image_analysis or {},\n",
        "        \"schema\": report.schema or {}\n",
        "    }, cls=DisasterJSONEncoder)\n",
        "\n",
        "    cursor.execute(\n",
        "        \"\"\"\n",
        "        INSERT OR REPLACE INTO analysis_results\n",
        "        (report_id, disaster_type, confidence_score, reliability_score,\n",
        "         alert_level, location_lat, location_lon, analysis_data)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "        (\n",
        "            report.report_id,\n",
        "            report.disaster_type.value if report.disaster_type else \"unknown\",  # FIXED: Use .value\n",
        "            report.confidence_score,\n",
        "            report.reliability_score,\n",
        "            report.alert_level.value if report.alert_level else 1,  # FIXED: Use .value\n",
        "            report.location.get(\"lat\", 0.0) if report.location else 0.0,\n",
        "            report.location.get(\"lon\", 0.0) if report.location else 0.0,\n",
        "            analysis_data,\n",
        "        ),\n",
        "    )\n",
        "    self.db_connection.commit()\n",
        "\n",
        "# Add these methods to the class\n",
        "OceanDisasterDetector.load_pending_reports = load_pending_reports\n",
        "OceanDisasterDetector._load_images_for_report = _load_images_for_report\n",
        "OceanDisasterDetector._update_report_status = _update_report_status\n",
        "OceanDisasterDetector._save_analysis_result = _save_analysis_result\n",
        "\n",
        "print(\"FIXED data loading methods with proper JSON serialization added to OceanDisasterDetector class!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMQqkYYTByTk",
        "outputId": "3fa6398b-77db-4e64-bba4-c0faaa31c289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIXED data loading methods with proper JSON serialization added to OceanDisasterDetector class!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Image Analysis Methods\n",
        "\n",
        "def extract_gps_from_images(self, images: List[str]) -> Optional[Dict[str, float]]:\n",
        "    if images is None:\n",
        "        return None\n",
        "\n",
        "    for image_data in images:\n",
        "        try:\n",
        "            if not image_data:\n",
        "                continue\n",
        "\n",
        "            image_bytes = base64.b64decode(image_data)\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            exifdata = image.getexif()\n",
        "\n",
        "            if exifdata is not None:\n",
        "                for tag_id in exifdata:\n",
        "                    tag = TAGS.get(tag_id, tag_id)\n",
        "                    if tag == \"GPSInfo\":\n",
        "                        gps_info = exifdata.get_ifd(tag_id)\n",
        "                        gps_data = self._parse_gps_data(gps_info)\n",
        "                        if gps_data:\n",
        "                            return gps_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting GPS from image: {e}\")\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "def _parse_gps_data(self, gps_info) -> Optional[Dict[str, float]]:\n",
        "    try:\n",
        "        def convert_to_degrees(value):\n",
        "            if not value or not isinstance(value, (list, tuple)) or safe_len(value) < 3:\n",
        "                return None\n",
        "            d, m, s = value\n",
        "            return d + (m / 60.0) + (s / 3600.0)\n",
        "\n",
        "        lat = None\n",
        "        lon = None\n",
        "\n",
        "        if 2 in gps_info and 1 in gps_info:\n",
        "            lat_degrees = convert_to_degrees(gps_info[2])\n",
        "            if lat_degrees is not None:\n",
        "                lat = lat_degrees\n",
        "                if gps_info[1] == \"S\":\n",
        "                    lat = -lat\n",
        "\n",
        "        if 4 in gps_info and 3 in gps_info:\n",
        "            lon_degrees = convert_to_degrees(gps_info[4])\n",
        "            if lon_degrees is not None:\n",
        "                lon = lon_degrees\n",
        "                if gps_info[3] == \"W\":\n",
        "                    lon = -lon\n",
        "\n",
        "        if lat is not None and lon is not None:\n",
        "            return {\"lat\": lat, \"lon\": lon}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing GPS data: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def analyze_images(self, images: List[str]) -> Dict:\n",
        "    analysis_results = {\n",
        "        \"total_images\": 0,\n",
        "        \"disaster_indicators\": [],\n",
        "        \"confidence\": 0.0,\n",
        "        \"visual_features\": [],\n",
        "        \"gps_extracted\": False,\n",
        "        \"gps_location\": None,\n",
        "    }\n",
        "\n",
        "    if images is None:\n",
        "        return analysis_results\n",
        "\n",
        "    analysis_results[\"total_images\"] = safe_len(images)\n",
        "\n",
        "    disaster_indicators = 0\n",
        "    total_confidence = 0.0\n",
        "\n",
        "    gps_location = self.extract_gps_from_images(images)\n",
        "    if gps_location:\n",
        "        analysis_results[\"gps_extracted\"] = True\n",
        "        analysis_results[\"gps_location\"] = gps_location\n",
        "\n",
        "    for i, image_data in enumerate(images):\n",
        "        try:\n",
        "            if not image_data:\n",
        "                analysis_results[\"disaster_indicators\"].append(\n",
        "                    {\"has_disaster\": False, \"confidence\": 0.0, \"error\": \"Empty image data\"}\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            image_bytes = base64.b64decode(image_data)\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            features = self._extract_visual_features(image)\n",
        "            analysis_results[\"visual_features\"].append(features)\n",
        "\n",
        "            indicators = self._detect_disaster_indicators(features)\n",
        "            if indicators[\"has_disaster\"]:\n",
        "                disaster_indicators += 1\n",
        "                total_confidence += indicators[\"confidence\"]\n",
        "\n",
        "            analysis_results[\"disaster_indicators\"].append(indicators)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing image {i}: {e}\")\n",
        "            analysis_results[\"disaster_indicators\"].append(\n",
        "                {\"has_disaster\": False, \"confidence\": 0.0, \"error\": str(e)}\n",
        "            )\n",
        "\n",
        "    if disaster_indicators > 0:\n",
        "        analysis_results[\"confidence\"] = total_confidence / disaster_indicators\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "def _extract_visual_features(self, image: Image) -> Dict:\n",
        "    if not image:\n",
        "        return {\n",
        "            \"mean_red\": 0.0, \"mean_green\": 0.0, \"mean_blue\": 0.0,\n",
        "            \"texture\": 0.0, \"edge_strength\": 0.0, \"brightness\": 0.0,\n",
        "        }\n",
        "\n",
        "    img_array = np.array(image.resize((224, 224)))\n",
        "\n",
        "    if safe_len(img_array.shape) == 2:\n",
        "        img_array = np.stack([img_array] * 3, axis=-1)\n",
        "    elif img_array.shape[-1] == 4:\n",
        "        img_array = img_array[:, :, :3]\n",
        "\n",
        "    mean_rgb = np.mean(img_array, axis=(0, 1))\n",
        "    gray = np.mean(img_array, axis=2)\n",
        "    texture = np.std(gray)\n",
        "    edges = np.sum(np.abs(np.gradient(gray)))\n",
        "\n",
        "    return {\n",
        "        \"mean_red\": float(mean_rgb[0]),\n",
        "        \"mean_green\": float(mean_rgb[1]) if safe_len(mean_rgb) > 1 else float(mean_rgb[0]),\n",
        "        \"mean_blue\": float(mean_rgb[2]) if safe_len(mean_rgb) > 2 else float(mean_rgb[0]),\n",
        "        \"texture\": float(texture),\n",
        "        \"edge_strength\": float(edges),\n",
        "        \"brightness\": float(np.mean(gray)),\n",
        "    }\n",
        "\n",
        "def _detect_disaster_indicators(self, features: Dict) -> Dict:\n",
        "    if not features or not isinstance(features, dict):\n",
        "        return {\n",
        "            \"has_disaster\": False,\n",
        "            \"confidence\": 0.0,\n",
        "            \"predicted_type\": DisasterType.UNKNOWN,\n",
        "        }\n",
        "\n",
        "    confidence = 0.0\n",
        "    disaster_type = DisasterType.UNKNOWN\n",
        "\n",
        "    brightness = features.get(\"brightness\", 0)\n",
        "    texture = features.get(\"texture\", 0)\n",
        "    edge_strength = features.get(\"edge_strength\", 0)\n",
        "\n",
        "    if brightness < 100 and texture > 50:\n",
        "        confidence += 0.3\n",
        "        disaster_type = DisasterType.TSUNAMI\n",
        "\n",
        "    if edge_strength > 10000:\n",
        "        confidence += 0.2\n",
        "\n",
        "    if (features.get(\"mean_red\", 0) > features.get(\"mean_blue\", 0) and\n",
        "        features.get(\"mean_green\", 0) > features.get(\"mean_blue\", 0)):\n",
        "        confidence += 0.2\n",
        "        disaster_type = DisasterType.COASTAL_FLOODING\n",
        "\n",
        "    if brightness < 80 and texture < 30:\n",
        "        confidence += 0.4\n",
        "        disaster_type = DisasterType.OIL_SPILL\n",
        "\n",
        "    return {\n",
        "        \"has_disaster\": confidence > 0.3,\n",
        "        \"confidence\": min(confidence, 1.0),\n",
        "        \"predicted_type\": disaster_type,\n",
        "    }\n",
        "\n",
        "# Add methods to class\n",
        "OceanDisasterDetector.extract_gps_from_images = extract_gps_from_images\n",
        "OceanDisasterDetector._parse_gps_data = _parse_gps_data\n",
        "OceanDisasterDetector.analyze_images = analyze_images\n",
        "OceanDisasterDetector._extract_visual_features = _extract_visual_features\n",
        "OceanDisasterDetector._detect_disaster_indicators = _detect_disaster_indicators\n",
        "\n",
        "print(\"Image analysis methods added to OceanDisasterDetector class!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srD13g-1B1PG",
        "outputId": "2789a03c-b4c2-44db-b703-4228adfb6781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image analysis methods added to OceanDisasterDetector class!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Text Analysis and Risk Assessment Methods\n",
        "\n",
        "def analyze_description(self, description: str) -> Tuple[DisasterType, float]:\n",
        "    if description is None or not isinstance(description, str):\n",
        "        return DisasterType.UNKNOWN, 0.0\n",
        "\n",
        "    desc_lower = description.lower().strip()\n",
        "    if not desc_lower:\n",
        "        return DisasterType.UNKNOWN, 0.0\n",
        "\n",
        "    best_type = DisasterType.UNKNOWN\n",
        "    best_score = 0.0\n",
        "\n",
        "    for dtype, keywords in self.disaster_keywords.items():\n",
        "        if keywords is None or not isinstance(keywords, list):\n",
        "            continue\n",
        "\n",
        "        matches = sum(1 for kw in keywords if kw and isinstance(kw, str) and kw in desc_lower)\n",
        "        score = matches / max(safe_len(keywords), 1) if keywords else 0.0\n",
        "\n",
        "        if score > best_score:\n",
        "            best_type = dtype\n",
        "            best_score = score\n",
        "\n",
        "    return best_type, best_score\n",
        "\n",
        "def calculate_reliability_score(\n",
        "    self,\n",
        "    description: str,\n",
        "    selected_disaster: DisasterType,\n",
        "    image_analysis: Dict,\n",
        "    location: Optional[Dict[str, float]],\n",
        ") -> float:\n",
        "    score = 0.0\n",
        "\n",
        "    if description and isinstance(description, str) and description.strip():\n",
        "        desc_score = min(safe_len(description.split()) / 20.0, 1.0)\n",
        "        score += desc_score * 0.3\n",
        "\n",
        "    if image_analysis and isinstance(image_analysis, dict) and image_analysis.get(\"confidence\", 0) > 0:\n",
        "        score += image_analysis[\"confidence\"] * 0.4\n",
        "\n",
        "    if location and isinstance(location, dict) and \"lat\" in location and \"lon\" in location:\n",
        "        if self._is_coastal_location(location):\n",
        "            score += 0.2\n",
        "        else:\n",
        "            score += 0.1\n",
        "\n",
        "    if location and isinstance(location, dict):\n",
        "        historical_score = self._check_historical_consistency(selected_disaster, location)\n",
        "        score += historical_score * 0.1\n",
        "\n",
        "    return min(score, 1.0)\n",
        "\n",
        "def _is_coastal_location(self, location: Dict[str, float]) -> bool:\n",
        "    if not location or not isinstance(location, dict):\n",
        "        return True  # Default assumption for coastal areas\n",
        "\n",
        "    if \"lat\" not in location or \"lon\" not in location:\n",
        "        return True\n",
        "\n",
        "    cursor = self.db_connection.cursor()\n",
        "    try:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT is_coastal FROM locations\n",
        "            WHERE ABS(latitude - ?) < 0.5 AND ABS(longitude - ?) < 0.5\n",
        "            AND is_coastal IS NOT NULL\n",
        "            LIMIT 1\n",
        "            \"\"\",\n",
        "            (location[\"lat\"], location[\"lon\"]),\n",
        "        )\n",
        "        result = cursor.fetchone()\n",
        "        if result:\n",
        "            return bool(result[0])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return True\n",
        "\n",
        "def _check_historical_consistency(\n",
        "    self, disaster_type: DisasterType, location: Dict[str, float]\n",
        ") -> float:\n",
        "    if not location or not isinstance(location, dict) or not disaster_type:\n",
        "        return 0.3\n",
        "\n",
        "    cursor = self.db_connection.cursor()\n",
        "    try:\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT disaster_type, COUNT(*) as count\n",
        "            FROM historical_disasters\n",
        "            WHERE ABS(latitude - ?) < 1.0 AND ABS(longitude - ?) < 1.0\n",
        "            GROUP BY disaster_type\n",
        "            \"\"\",\n",
        "            (location.get(\"lat\", 0), location.get(\"lon\", 0)),\n",
        "        )\n",
        "        results = cursor.fetchall()\n",
        "        total_records = sum(count for _, count in results)\n",
        "\n",
        "        if total_records == 0:\n",
        "            return 0.5\n",
        "\n",
        "        for db_disaster_type, count in results:\n",
        "            if db_disaster_type == disaster_type.value:\n",
        "                return count / total_records\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0.3\n",
        "\n",
        "def determine_alert_level(\n",
        "    self,\n",
        "    disaster_type: DisasterType,\n",
        "    confidence: float,\n",
        "    reliability: float,\n",
        "    image_analysis: Dict,\n",
        ") -> AlertLevel:\n",
        "    if not disaster_type:\n",
        "        disaster_type = DisasterType.UNKNOWN\n",
        "\n",
        "    confidence = confidence if isinstance(confidence, (int, float)) else 0.0\n",
        "    reliability = reliability if isinstance(reliability, (int, float)) else 0.0\n",
        "    image_analysis = image_analysis if isinstance(image_analysis, dict) else {}\n",
        "\n",
        "    severity_mapping = {\n",
        "        DisasterType.TSUNAMI: 3,\n",
        "        DisasterType.HURRICANE: 2,\n",
        "        DisasterType.STORM_SURGE: 2,\n",
        "        DisasterType.COASTAL_FLOODING: 1,\n",
        "        DisasterType.RIP_CURRENT: 1,\n",
        "        DisasterType.HIGH_WAVES: 1,\n",
        "        DisasterType.ALGAE_BLOOM: 1,\n",
        "        DisasterType.OIL_SPILL: 2,\n",
        "        DisasterType.EROSION: 1,\n",
        "        DisasterType.UNKNOWN: 1,\n",
        "    }\n",
        "\n",
        "    base_severity = severity_mapping.get(disaster_type, 1)\n",
        "    combined_score = (confidence + reliability) / 2\n",
        "\n",
        "    if combined_score < 0.4:\n",
        "        adjusted_severity = max(1, base_severity - 1)\n",
        "    elif combined_score > 0.8:\n",
        "        if base_severity < 3 and disaster_type in [DisasterType.HURRICANE, DisasterType.STORM_SURGE]:\n",
        "            adjusted_severity = base_severity + 1\n",
        "        else:\n",
        "            adjusted_severity = base_severity\n",
        "    else:\n",
        "        adjusted_severity = base_severity\n",
        "\n",
        "    if image_analysis and image_analysis.get(\"confidence\", 0) > 0.7:\n",
        "        adjusted_severity = min(3, adjusted_severity + 1)\n",
        "\n",
        "    return AlertLevel(adjusted_severity)\n",
        "\n",
        "def _get_recommended_actions(\n",
        "    self, disaster_type: DisasterType, alert_level: AlertLevel\n",
        ") -> List[str]:\n",
        "    if not disaster_type:\n",
        "        disaster_type = DisasterType.UNKNOWN\n",
        "    if not alert_level:\n",
        "        alert_level = AlertLevel.WARNING\n",
        "\n",
        "    base_actions = {\n",
        "        DisasterType.TSUNAMI: {\n",
        "            AlertLevel.WARNING: [\"Monitor official alerts\", \"Review evacuation routes\"],\n",
        "            AlertLevel.EMERGENCY: [\"Move to higher ground immediately\", \"Stay away from beaches\"],\n",
        "            AlertLevel.EVACUATION: [\"EVACUATE IMMEDIATELY to high ground\", \"Do not return until all-clear\"],\n",
        "        },\n",
        "        DisasterType.HURRICANE: {\n",
        "            AlertLevel.WARNING: [\"Secure outdoor items\", \"Stock emergency supplies\"],\n",
        "            AlertLevel.EMERGENCY: [\"Stay indoors\", \"Avoid coastal areas\"],\n",
        "            AlertLevel.EVACUATION: [\"EVACUATE coastal and flood-prone areas\", \"Follow official evacuation orders\"],\n",
        "        },\n",
        "        DisasterType.STORM_SURGE: {\n",
        "            AlertLevel.WARNING: [\"Monitor weather updates\", \"Prepare for possible flooding\"],\n",
        "            AlertLevel.EMERGENCY: [\"Move away from low-lying areas\", \"Secure property\"],\n",
        "            AlertLevel.EVACUATION: [\"EVACUATE flood-prone areas immediately\", \"Seek higher ground\"],\n",
        "        },\n",
        "        DisasterType.OIL_SPILL: {\n",
        "            AlertLevel.WARNING: [\"Avoid affected water areas\", \"Report wildlife impacts\"],\n",
        "            AlertLevel.EMERGENCY: [\"Do not enter contaminated areas\", \"Seek medical attention if exposed\"],\n",
        "            AlertLevel.EVACUATION: [\"EVACUATE immediately if fumes present\", \"Follow health authority guidance\"],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    default_actions = {\n",
        "        AlertLevel.WARNING: [\"Stay alert\", \"Monitor official channels\"],\n",
        "        AlertLevel.EMERGENCY: [\"Take immediate precautions\", \"Avoid affected areas\"],\n",
        "        AlertLevel.EVACUATION: [\"EVACUATE if instructed by authorities\"],\n",
        "    }\n",
        "\n",
        "    return base_actions.get(disaster_type, default_actions).get(\n",
        "        alert_level, default_actions[AlertLevel.WARNING]\n",
        "    )\n",
        "\n",
        "# Add methods to class\n",
        "OceanDisasterDetector.analyze_description = analyze_description\n",
        "OceanDisasterDetector.calculate_reliability_score = calculate_reliability_score\n",
        "OceanDisasterDetector._is_coastal_location = _is_coastal_location\n",
        "OceanDisasterDetector._check_historical_consistency = _check_historical_consistency\n",
        "OceanDisasterDetector.determine_alert_level = determine_alert_level\n",
        "OceanDisasterDetector._get_recommended_actions = _get_recommended_actions\n",
        "\n",
        "print(\"Text analysis and risk assessment methods added to OceanDisasterDetector class!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgMODNJkCBB_",
        "outputId": "19fe9267-bf73-46e9-fedb-43c08e7d217b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis and risk assessment methods added to OceanDisasterDetector class!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Report Processing and Schema Creation\n",
        "\n",
        "def create_schema(self, report: DisasterReport) -> Dict:\n",
        "    if not report:\n",
        "        return {}\n",
        "\n",
        "    return {\n",
        "        \"event_id\": report.report_id or f\"{report.disaster_type.value}_{int(report.timestamp.timestamp())}\",\n",
        "        \"disaster_type\": report.disaster_type.value,\n",
        "        \"alert_level\": report.alert_level.value,\n",
        "        \"alert_name\": report.alert_level.name,\n",
        "        \"confidence_score\": round(report.confidence_score, 3),\n",
        "        \"reliability_score\": round(report.reliability_score, 3),\n",
        "        \"location\": report.location or {\"lat\": 0.0, \"lon\": 0.0},\n",
        "        \"timestamp\": report.timestamp.isoformat(),\n",
        "        \"description\": report.description or \"\",\n",
        "        \"image_analysis\": report.image_analysis or {},\n",
        "        \"risk_assessment\": {\n",
        "            \"immediate_danger\": report.alert_level.value >= 2,\n",
        "            \"evacuation_needed\": report.alert_level.value == 3,\n",
        "            \"monitoring_required\": True,\n",
        "        },\n",
        "        \"recommended_actions\": self._get_recommended_actions(report.disaster_type, report.alert_level),\n",
        "    }\n",
        "\n",
        "def process_report(self, data: Dict) -> DisasterReport:\n",
        "    if not data or not isinstance(data, dict):\n",
        "        data = {}\n",
        "\n",
        "    description = data.get(\"description\", \"\") or \"\"\n",
        "    images = data.get(\"images\", []) or []\n",
        "    selected_disaster = data.get(\"selected_disaster\")\n",
        "    provided_location = data.get(\"location\")\n",
        "    report_id = data.get(\"report_id\")\n",
        "\n",
        "    # Ensure images is never None\n",
        "    if images is None:\n",
        "        images = []\n",
        "\n",
        "    location = None\n",
        "    image_analysis: Dict[str, Any] = {}\n",
        "\n",
        "    if images and isinstance(images, list):\n",
        "        image_analysis = self.analyze_images(images)\n",
        "        if image_analysis.get(\"gps_extracted\") and image_analysis.get(\"gps_location\"):\n",
        "            location = image_analysis[\"gps_location\"]\n",
        "\n",
        "    if not location and provided_location and isinstance(provided_location, dict):\n",
        "        location = provided_location\n",
        "\n",
        "    if not location:\n",
        "        location = {\"lat\": 0.0, \"lon\": 0.0}\n",
        "\n",
        "    if selected_disaster:\n",
        "        try:\n",
        "            disaster_type = DisasterType(selected_disaster)\n",
        "            desc_confidence = 1.0 if description and description.strip() else 0.5\n",
        "        except ValueError:\n",
        "            disaster_type = DisasterType.UNKNOWN\n",
        "            desc_confidence = 0.3\n",
        "    else:\n",
        "        disaster_type, desc_confidence = self.analyze_description(description)\n",
        "\n",
        "    confidence_score = desc_confidence\n",
        "    if image_analysis and isinstance(image_analysis, dict) and \"confidence\" in image_analysis:\n",
        "        confidence_score = (confidence_score + image_analysis[\"confidence\"]) / 2\n",
        "\n",
        "    reliability_score = self.calculate_reliability_score(\n",
        "        description, disaster_type, image_analysis, location\n",
        "    )\n",
        "\n",
        "    alert_level = self.determine_alert_level(\n",
        "        disaster_type, confidence_score, reliability_score, image_analysis\n",
        "    )\n",
        "\n",
        "    report = DisasterReport(\n",
        "        disaster_type=disaster_type,\n",
        "        confidence_score=confidence_score,\n",
        "        reliability_score=reliability_score,\n",
        "        alert_level=alert_level,\n",
        "        location=location,\n",
        "        timestamp=datetime.now(),\n",
        "        description=description,\n",
        "        image_analysis=image_analysis,\n",
        "        schema={},\n",
        "        report_id=report_id,\n",
        "    )\n",
        "\n",
        "    report.schema = self.create_schema(report)\n",
        "    return report\n",
        "\n",
        "def process_all_reports(self) -> List[DisasterReport]:\n",
        "    print(\"Loading pending reports from database...\")\n",
        "    pending_reports = self.load_pending_reports()\n",
        "\n",
        "    if not pending_reports:\n",
        "        print(\"No pending reports found in database\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Found {len(pending_reports)} pending reports\")\n",
        "    processed_reports: List[DisasterReport] = []\n",
        "\n",
        "    for i, report_data in enumerate(pending_reports):\n",
        "        try:\n",
        "            print(f\"Processing report {i+1}/{len(pending_reports)}: {report_data.get('report_id', 'unknown')}\")\n",
        "            report = self.process_report(report_data)\n",
        "            processed_reports.append(report)\n",
        "            self._update_report_status(report_data.get(\"report_id\"), \"processed\")\n",
        "            self._save_analysis_result(report)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing report {report_data.get('report_id', 'unknown')}: {e}\")\n",
        "            self._update_report_status(report_data.get(\"report_id\"), \"error\", str(e))\n",
        "\n",
        "    return processed_reports\n",
        "\n",
        "# Add methods to class\n",
        "OceanDisasterDetector.create_schema = create_schema\n",
        "OceanDisasterDetector.process_report = process_report\n",
        "OceanDisasterDetector.process_all_reports = process_all_reports\n",
        "\n",
        "print(\"Report processing methods added!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAg-RikCCSnm",
        "outputId": "d4905506-32e6-4696-bbb4-64d4edc56950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report processing methods added!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 FIXED: Output and Main Analysis Functions with proper JSON serialization\n",
        "\n",
        "def save_results_to_json(self, reports: List[DisasterReport]) -> str:\n",
        "    \"\"\"FIXED: Save results with proper enum serialization\"\"\"\n",
        "    if reports is None:\n",
        "        reports = []\n",
        "\n",
        "    output_data: Dict[str, Any] = {\n",
        "        \"total_reports\": safe_len(reports),\n",
        "        \"summary\": {\n",
        "            \"by_disaster_type\": {},\n",
        "            \"by_alert_level\": {},\n",
        "            \"average_confidence\": 0.0,\n",
        "            \"average_reliability\": 0.0,\n",
        "        },\n",
        "        \"reports\": [],\n",
        "    }\n",
        "\n",
        "    total_confidence = 0.0\n",
        "    total_reliability = 0.0\n",
        "\n",
        "    for report in reports:\n",
        "        if not report:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Use the fixed to_dict method that handles enum serialization\n",
        "            report_dict = report.to_dict()\n",
        "            output_data[\"reports\"].append(report_dict)\n",
        "\n",
        "            # Get serialized values instead of enum objects\n",
        "            disaster_type = report.disaster_type.value if report.disaster_type else \"unknown\"\n",
        "            alert_level = report.alert_level.name if report.alert_level else \"UNKNOWN\"\n",
        "\n",
        "            if disaster_type not in output_data[\"summary\"][\"by_disaster_type\"]:\n",
        "                output_data[\"summary\"][\"by_disaster_type\"][disaster_type] = 0\n",
        "            if alert_level not in output_data[\"summary\"][\"by_alert_level\"]:\n",
        "                output_data[\"summary\"][\"by_alert_level\"][alert_level] = 0\n",
        "\n",
        "            output_data[\"summary\"][\"by_disaster_type\"][disaster_type] += 1\n",
        "            output_data[\"summary\"][\"by_alert_level\"][alert_level] += 1\n",
        "\n",
        "            total_confidence += getattr(report, \"confidence_score\", 0.0)\n",
        "            total_reliability += getattr(report, \"reliability_score\", 0.0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing report for JSON output: {e}\")\n",
        "            continue\n",
        "\n",
        "    if reports:\n",
        "        output_data[\"summary\"][\"average_confidence\"] = round(total_confidence / safe_len(reports), 3)\n",
        "        output_data[\"summary\"][\"average_reliability\"] = round(total_reliability / safe_len(reports), 3)\n",
        "\n",
        "    try:\n",
        "        # FIXED: Use custom JSON encoder to handle any remaining enum issues\n",
        "        with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False, cls=DisasterJSONEncoder)\n",
        "        print(f\"Results saved to {self.output_path}\")\n",
        "        return self.output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results to JSON: {e}\")\n",
        "        backup_path = \"backup_disaster_results.json\"\n",
        "        try:\n",
        "            # FIXED: Use custom JSON encoder for backup too\n",
        "            with open(backup_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(output_data, f, indent=2, ensure_ascii=False, cls=DisasterJSONEncoder)\n",
        "            print(f\"Results saved to backup location: {backup_path}\")\n",
        "            return backup_path\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to save to backup location: {e2}\")\n",
        "            return \"\"\n",
        "\n",
        "def run_complete_analysis(self) -> Optional[str]:\n",
        "    print(\"Starting Ocean Disaster Detection Analysis...\")\n",
        "    print(f\"Database: {self.database_path}\")\n",
        "    print(f\"Output: {self.output_path}\")\n",
        "\n",
        "    try:\n",
        "        reports = self.process_all_reports()\n",
        "        if not reports:\n",
        "            print(\"No reports were processed.\")\n",
        "            return None\n",
        "\n",
        "        output_file = self.save_results_to_json(reports)\n",
        "        if output_file:\n",
        "            print(\"Analysis complete.\")\n",
        "            return output_file\n",
        "        else:\n",
        "            print(\"Analysis completed but failed to save results.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error during complete analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        self.close_connection()\n",
        "\n",
        "# Add methods to class\n",
        "OceanDisasterDetector.save_results_to_json = save_results_to_json\n",
        "OceanDisasterDetector.run_complete_analysis = run_complete_analysis\n",
        "\n",
        "print(\"FIXED output and main analysis methods with proper JSON serialization added!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtgKKEH9CVtl",
        "outputId": "65be8e87-3208-4fbd-fbfc-19c358744ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIXED output and main analysis methods with proper JSON serialization added!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Database Setup Utility Class\n",
        "\n",
        "class DatabaseSetup:\n",
        "    \"\"\"Utility class to help users set up their database with sample data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_sample_database(db_path: str = \"ocean_disasters.db\"):\n",
        "        try:\n",
        "            if os.path.exists(db_path):\n",
        "                os.remove(db_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not remove existing database: {e}\")\n",
        "\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create schema using a temporary detector bound to this connection\n",
        "        temp = object.__new__(OceanDisasterDetector)\n",
        "        temp.db_connection = conn\n",
        "        OceanDisasterDetector._create_expected_schema(temp)\n",
        "\n",
        "        sample_reports = [\n",
        "            {\n",
        "                \"report_id\": \"RPT001\",\n",
        "                \"description\": \"Large waves hitting the shore, water rushing inland rapidly. Houses near beach are flooding.\",\n",
        "                \"disaster_type\": \"tsunami\",\n",
        "                \"timestamp\": \"2024-01-15T10:30:00\",\n",
        "                \"user_id\": \"user123\",\n",
        "            },\n",
        "            {\n",
        "                \"report_id\": \"RPT002\",\n",
        "                \"description\": \"Strong winds and storm surge causing coastal flooding in low-lying areas.\",\n",
        "                \"disaster_type\": \"storm_surge\",\n",
        "                \"timestamp\": \"2024-01-16T14:20:00\",\n",
        "                \"user_id\": \"user456\",\n",
        "            },\n",
        "            {\n",
        "                \"report_id\": \"RPT003\",\n",
        "                \"description\": \"Black oily substance covering the water surface, dead fish washing ashore.\",\n",
        "                \"disaster_type\": \"oil_spill\",\n",
        "                \"timestamp\": \"2024-01-17T09:15:00\",\n",
        "                \"user_id\": \"user789\",\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        for report in sample_reports:\n",
        "            try:\n",
        "                cursor.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO reports (report_id, description, disaster_type, timestamp, user_id)\n",
        "                    VALUES (?, ?, ?, ?, ?)\n",
        "                    \"\"\",\n",
        "                    (\n",
        "                        report[\"report_id\"],\n",
        "                        report[\"description\"],\n",
        "                        report[\"disaster_type\"],\n",
        "                        report[\"timestamp\"],\n",
        "                        report[\"user_id\"],\n",
        "                    ),\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting sample report {report['report_id']}: {e}\")\n",
        "\n",
        "        sample_locations = [\n",
        "            (\"RPT001\", 13.0827, 80.2707, \"manual\", \"Chennai Beach, Tamil Nadu\", 1),\n",
        "            (\"RPT002\", 19.0760, 72.8777, \"gps\", \"Mumbai Coastline, Maharashtra\", 1),\n",
        "            (\"RPT003\", 15.2993, 74.1240, \"manual\", \"Goa Coast\", 1),\n",
        "        ]\n",
        "\n",
        "        for loc in sample_locations:\n",
        "            try:\n",
        "                cursor.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO locations (report_id, latitude, longitude, location_source, address, is_coastal)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?)\n",
        "                    \"\"\",\n",
        "                    loc,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting sample location for {loc[0]}: {e}\")\n",
        "\n",
        "        historical_data = [\n",
        "            (\"tsunami\", 13.0827, 80.2707, \"2004 Indian Ocean Tsunami\", 3, \"2004-12-26T00:58:53\"),\n",
        "            (\"hurricane\", 19.0760, 72.8777, \"Cyclone Tauktae impact\", 2, \"2021-05-17T18:00:00\"),\n",
        "            (\"oil_spill\", 15.2993, 74.1240, \"Merchant vessel oil leak\", 2, \"2023-03-10T12:30:00\"),\n",
        "        ]\n",
        "\n",
        "        for hist in historical_data:\n",
        "            try:\n",
        "                cursor.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO historical_disasters (disaster_type, latitude, longitude, description, severity, timestamp)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?)\n",
        "                    \"\"\",\n",
        "                    hist,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting historical data: {e}\")\n",
        "\n",
        "        sample_users = [\n",
        "            (\"user123\", \"reporter1\", \"user1@example.com\", 0.8, 5, 4),\n",
        "            (\"user456\", \"reporter2\", \"user2@example.com\", 0.6, 3, 2),\n",
        "            (\"user789\", \"reporter3\", \"user3@example.com\", 0.9, 7, 6),\n",
        "        ]\n",
        "\n",
        "        for user in sample_users:\n",
        "            try:\n",
        "                cursor.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO users (user_id, username, email, reliability_score, total_reports, verified_reports)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?)\n",
        "                    \"\"\",\n",
        "                    user,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting sample user {user[0]}: {e}\")\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        print(f\"Sample database created at: {db_path}\")\n",
        "        print(\"Sample data includes 3 reports with locations and historical context\")\n",
        "        return db_path\n",
        "\n",
        "    @staticmethod\n",
        "    def add_sample_images(db_path: str):\n",
        "        try:\n",
        "            conn = sqlite3.connect(db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            def create_sample_image(color_rgb):\n",
        "                try:\n",
        "                    img = Image.new(\"RGB\", (100, 100), color=color_rgb)\n",
        "                    buffer = io.BytesIO()\n",
        "                    img.save(buffer, format=\"JPEG\")\n",
        "                    img_bytes = buffer.getvalue()\n",
        "                    return base64.b64encode(img_bytes).decode(\"utf-8\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating sample image: {e}\")\n",
        "                    return \"\"\n",
        "\n",
        "            sample_images = [\n",
        "                (\"RPT001\", create_sample_image((50, 100, 150)), \"tsunami_wave.jpg\", 0, None, None),\n",
        "                (\"RPT002\", create_sample_image((100, 100, 100)), \"storm_surge.jpg\", 1, 19.0760, 72.8777),\n",
        "                (\"RPT003\", create_sample_image((20, 20, 20)), \"oil_spill.jpg\", 0, None, None),\n",
        "            ]\n",
        "\n",
        "            for img_data in sample_images:\n",
        "                if not img_data[1]:  # If image_data is empty\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    cursor.execute(\n",
        "                        \"\"\"\n",
        "                        INSERT INTO images (report_id, image_data, filename, has_gps, gps_lat, gps_lon)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?)\n",
        "                        \"\"\",\n",
        "                        img_data,\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error inserting sample image for {img_data[0]}: {e}\")\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "            print(\"Sample images added to database\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding sample images: {e}\")\n",
        "\n",
        "print(\"DatabaseSetup class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifAPJ7d5CZF_",
        "outputId": "afa00360-7cef-45b4-8d49-02a5c2626932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatabaseSetup class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Usage Functions and Testing\n",
        "\n",
        "def run_analysis_from_database(database_path: str, output_path: str = \"results.json\") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Convenience function to run analysis from a database path.\n",
        "\n",
        "    Args:\n",
        "        database_path: Path to the SQLite database\n",
        "        output_path: Path for the output JSON file\n",
        "\n",
        "    Returns:\n",
        "        Path to the results file if successful, None otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        detector = OceanDisasterDetector(database_path, output_path)\n",
        "        return detector.run_complete_analysis()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in run_analysis_from_database: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_test_database_and_run():\n",
        "    \"\"\"\n",
        "    Create a test database with sample data and run analysis\n",
        "    \"\"\"\n",
        "    print(\"Creating test database with sample data...\")\n",
        "\n",
        "    # Create sample database\n",
        "    db_path = DatabaseSetup.create_sample_database(\"test_ocean_disasters.db\")\n",
        "    DatabaseSetup.add_sample_images(db_path)\n",
        "\n",
        "    print(\"\\nRunning analysis on test database...\")\n",
        "\n",
        "    # Run analysis\n",
        "    result_file = run_analysis_from_database(db_path, \"test_results.json\")\n",
        "\n",
        "    if result_file:\n",
        "        print(f\"\\n✅ Analysis completed successfully!\")\n",
        "        print(f\"📊 Results saved to: {result_file}\")\n",
        "\n",
        "        # Display results summary\n",
        "        try:\n",
        "            with open(result_file, \"r\") as f:\n",
        "                results = json.load(f)\n",
        "                print(\"\\n📈 Summary:\")\n",
        "                print(f\"   Total Reports: {results.get('total_reports', 0)}\")\n",
        "                print(f\"   Average Confidence: {results.get('summary', {}).get('average_confidence', 0)}\")\n",
        "                print(f\"   Average Reliability: {results.get('summary', {}).get('average_reliability', 0)}\")\n",
        "                print(f\"   Alert Levels: {results.get('summary', {}).get('by_alert_level', {})}\")\n",
        "                print(f\"   Disaster Types: {results.get('summary', {}).get('by_disaster_type', {})}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading results file: {e}\")\n",
        "    else:\n",
        "        print(\"❌ Analysis failed or no results generated.\")\n",
        "\n",
        "    return result_file\n",
        "\n",
        "def test_individual_components():\n",
        "    \"\"\"\n",
        "    Test individual components of the system\n",
        "    \"\"\"\n",
        "    print(\"Testing individual components...\")\n",
        "\n",
        "    # Test safe_len function\n",
        "    print(f\"safe_len(None): {safe_len(None)}\")\n",
        "    print(f\"safe_len([]): {safe_len([])}\")\n",
        "    print(f\"safe_len([1,2,3]): {safe_len([1, 2, 3])}\")\n",
        "\n",
        "    # Test disaster type enumeration\n",
        "    print(f\"Disaster types: {[dt.value for dt in DisasterType]}\")\n",
        "\n",
        "    # Test alert levels\n",
        "    print(f\"Alert levels: {[al.name for al in AlertLevel]}\")\n",
        "\n",
        "    print(\"Component tests completed!\")\n",
        "\n",
        "# Make functions available\n",
        "print(\"Usage functions defined!\")\n",
        "print(\"\\nTo run the complete test:\")\n",
        "print(\"result = create_test_database_and_run()\")\n",
        "print(\"\\nTo test individual components:\")\n",
        "print(\"test_individual_components()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuAHRi-KChCE",
        "outputId": "c67544fc-2db8-4bc7-fe26-caafa19b45ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage functions defined!\n",
            "\n",
            "To run the complete test:\n",
            "result = create_test_database_and_run()\n",
            "\n",
            "To test individual components:\n",
            "test_individual_components()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Run Complete Test\n",
        "\n",
        "# First, test individual components to make sure everything is working\n",
        "test_individual_components()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RUNNING COMPLETE OCEAN DISASTER ANALYSIS TEST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Run the complete test\n",
        "try:\n",
        "    result_file = create_test_database_and_run()\n",
        "\n",
        "    if result_file:\n",
        "        print(f\"\\n🎉 SUCCESS: Analysis completed without the NoneType error!\")\n",
        "        print(f\"Results file: {result_file}\")\n",
        "\n",
        "        # Show some sample results\n",
        "        try:\n",
        "            with open(result_file, 'r') as f:\n",
        "                results = json.load(f)\n",
        "\n",
        "            print(f\"\\n📋 DETAILED RESULTS:\")\n",
        "            print(f\"📊 Total reports processed: {results['total_reports']}\")\n",
        "\n",
        "            if results['reports']:\n",
        "                print(f\"\\n🔍 Sample Report Analysis:\")\n",
        "                sample_report = results['reports'][0]\n",
        "                print(f\"   Report ID: {sample_report.get('report_id', 'N/A')}\")\n",
        "                print(f\"   Disaster Type: {sample_report.get('disaster_type', 'N/A')}\")\n",
        "                print(f\"   Alert Level: {sample_report.get('alert_level_name', 'N/A')}\")\n",
        "                print(f\"   Confidence: {sample_report.get('confidence_score', 0):.3f}\")\n",
        "                print(f\"   Reliability: {sample_report.get('reliability_score', 0):.3f}\")\n",
        "\n",
        "                # Show recommended actions\n",
        "                schema = sample_report.get('schema', {})\n",
        "                actions = schema.get('recommended_actions', [])\n",
        "                if actions:\n",
        "                    print(f\"   Recommended Actions: {', '.join(actions[:2])}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading detailed results: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ Test failed - check error messages above\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Test failed with error: {e}\")\n",
        "    print(\"This might be the NoneType error we're trying to fix.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TEST COMPLETED\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHgHxf8dClbg",
        "outputId": "78dbe3dc-f465-4b87-e15a-a118b1305acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing individual components...\n",
            "safe_len(None): 0\n",
            "safe_len([]): 0\n",
            "safe_len([1,2,3]): 3\n",
            "Disaster types: ['tsunami', 'hurricane', 'storm_surge', 'coastal_flooding', 'rip_current', 'high_waves', 'algae_bloom', 'oil_spill', 'coastal_erosion', 'unknown']\n",
            "Alert levels: ['WARNING', 'EMERGENCY', 'EVACUATION']\n",
            "Component tests completed!\n",
            "\n",
            "==================================================\n",
            "RUNNING COMPLETE OCEAN DISASTER ANALYSIS TEST\n",
            "==================================================\n",
            "Creating test database with sample data...\n",
            "Database schema created successfully\n",
            "Sample database created at: test_ocean_disasters.db\n",
            "Sample data includes 3 reports with locations and historical context\n",
            "Sample images added to database\n",
            "\n",
            "Running analysis on test database...\n",
            "Database schema validated successfully\n",
            "Starting Ocean Disaster Detection Analysis...\n",
            "Database: test_ocean_disasters.db\n",
            "Output: test_results.json\n",
            "Loading pending reports from database...\n",
            "Found 3 pending reports\n",
            "Processing report 1/3: RPT001\n",
            "Processing report 2/3: RPT002\n",
            "Processing report 3/3: RPT003\n",
            "Results saved to test_results.json\n",
            "Analysis complete.\n",
            "\n",
            "✅ Analysis completed successfully!\n",
            "📊 Results saved to: test_results.json\n",
            "\n",
            "📈 Summary:\n",
            "   Total Reports: 3\n",
            "   Average Confidence: 0.567\n",
            "   Average Reliability: 0.51\n",
            "   Alert Levels: {'EVACUATION': 1, 'EMERGENCY': 2}\n",
            "   Disaster Types: {'tsunami': 1, 'storm_surge': 1, 'oil_spill': 1}\n",
            "\n",
            "🎉 SUCCESS: Analysis completed without the NoneType error!\n",
            "Results file: test_results.json\n",
            "\n",
            "📋 DETAILED RESULTS:\n",
            "📊 Total reports processed: 3\n",
            "\n",
            "🔍 Sample Report Analysis:\n",
            "   Report ID: RPT001\n",
            "   Disaster Type: tsunami\n",
            "   Alert Level: EVACUATION\n",
            "   Confidence: 0.500\n",
            "   Reliability: 0.510\n",
            "   Recommended Actions: EVACUATE IMMEDIATELY to high ground, Do not return until all-clear\n",
            "\n",
            "==================================================\n",
            "TEST COMPLETED\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBkdhJdtEY6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}